{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9768c874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Peter and the Wolf: Reinforcement Learning Notebook\n",
    "\n",
    "# --- Imports ---\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3cd4de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RL Agent Wins: 24/100\n",
      "Random Walk Wins: 11/100\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- Environment Setup ---\n",
    "class Board:\n",
    "    def __init__(self, width=6, height=6):\n",
    "        self.width = width\n",
    "        self.height = height\n",
    "        self.grid = self.create_board()\n",
    "    \n",
    "    def create_board(self):\n",
    "        # Initialize empty ground\n",
    "        board = [['ground' for _ in range(self.width)] for _ in range(self.height)]\n",
    "        \n",
    "        # Randomly place apples, trees/grass, wolf\n",
    "        for _ in range(3):\n",
    "            x, y = random.randint(0, self.width-1), random.randint(0, self.height-1)\n",
    "            board[x][y] = 'apple'\n",
    "        for _ in range(3):\n",
    "            x, y = random.randint(0, self.width-1), random.randint(0, self.height-1)\n",
    "            board[x][y] = 'tree'\n",
    "        x, y = random.randint(0, self.width-1), random.randint(0, self.height-1)\n",
    "        board[x][y] = 'wolf'\n",
    "        \n",
    "        return board\n",
    "    \n",
    "    def render(self):\n",
    "        for row in self.grid:\n",
    "            print(' | '.join(row))\n",
    "        print()\n",
    "\n",
    "# --- Parameters ---\n",
    "actions = {0: (-1,0), 1: (1,0), 2: (0,-1), 3: (0,1)} # up, down, left, right\n",
    "max_energy = 20\n",
    "max_fatigue = 20\n",
    "required_energy = 10\n",
    "max_fatigue_for_battle = 5\n",
    "\n",
    "# Discretize energy/fatigue for Q-table\n",
    "n_energy_levels = 5\n",
    "n_fatigue_levels = 5\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def discretize(value, max_value, levels):\n",
    "    \"\"\"Map value to discrete index for Q-table\"\"\"\n",
    "    idx = int((value / max_value) * (levels - 1))\n",
    "    return max(0, min(levels - 1, idx))\n",
    "\n",
    "def reward(state, next_state, board):\n",
    "    x, y, energy, fatigue = next_state\n",
    "    r = -1  # penalty for moving\n",
    "    \n",
    "    cell = board.grid[x][y]\n",
    "    \n",
    "    if cell == 'apple':\n",
    "        r += 10\n",
    "        energy = min(energy + 5, max_energy)\n",
    "    if cell in ['tree', 'grass']:\n",
    "        r += 5\n",
    "        fatigue = max(fatigue - 5, 0)\n",
    "    if cell == 'wolf':\n",
    "        if energy >= required_energy and fatigue <= max_fatigue_for_battle:\n",
    "            r += 50\n",
    "        else:\n",
    "            r -= 50\n",
    "    return r\n",
    "\n",
    "def step(state, action, board):\n",
    "    x, y, energy, fatigue = state\n",
    "    dx, dy = actions[action]\n",
    "    new_x = max(0, min(board.width-1, x + dx))\n",
    "    new_y = max(0, min(board.height-1, y + dy))\n",
    "    \n",
    "    # Energy and fatigue updates\n",
    "    energy -= 1\n",
    "    fatigue += 1\n",
    "    \n",
    "    cell = board.grid[new_x][new_y]\n",
    "    if cell == 'apple':\n",
    "        energy = min(energy + 5, max_energy)\n",
    "    if cell in ['tree', 'grass']:\n",
    "        fatigue = max(fatigue - 5, 0)\n",
    "    \n",
    "    next_state = (new_x, new_y, energy, fatigue)\n",
    "    r = reward(state, next_state, board)\n",
    "    \n",
    "    return next_state, r\n",
    "\n",
    "def choose_action(Q, state, epsilon=0.1):\n",
    "    x, y, e, f = state\n",
    "    e_idx = discretize(e, max_energy, n_energy_levels)\n",
    "    f_idx = discretize(f, max_fatigue, n_fatigue_levels)\n",
    "    \n",
    "    if random.random() < epsilon:\n",
    "        return random.choice(list(actions.keys()))\n",
    "    else:\n",
    "        return np.argmax(Q[x, y, e_idx, f_idx, :])\n",
    "\n",
    "# --- Q-Learning Setup ---\n",
    "def q_learning(board, epochs=5000, alpha=0.1, gamma=0.9, epsilon=0.1):\n",
    "    Q = np.zeros((board.width, board.height, n_energy_levels, n_fatigue_levels, len(actions)))\n",
    "    \n",
    "    for ep in range(epochs):\n",
    "        # Random initial state\n",
    "        x, y = random.randint(0, board.width-1), random.randint(0, board.height-1)\n",
    "        state = (x, y, max_energy, 0)\n",
    "        total_reward = 0\n",
    "        done = False\n",
    "        steps = 0\n",
    "        \n",
    "        while not done and steps < 50:  # limit steps per episode\n",
    "            action = choose_action(Q, state, epsilon)\n",
    "            next_state, r = step(state, action, board)\n",
    "            \n",
    "            # Update Q-Table\n",
    "            x, y, e, f = state\n",
    "            new_x, new_y, new_e, new_f = next_state\n",
    "            e_idx = discretize(e, max_energy, n_energy_levels)\n",
    "            f_idx = discretize(f, max_fatigue, n_fatigue_levels)\n",
    "            new_e_idx = discretize(new_e, max_energy, n_energy_levels)\n",
    "            new_f_idx = discretize(new_f, max_fatigue, n_fatigue_levels)\n",
    "            \n",
    "            Q[x, y, e_idx, f_idx, action] = (1 - alpha) * Q[x, y, e_idx, f_idx, action] + \\\n",
    "                                            alpha * (r + gamma * np.max(Q[new_x, new_y, new_e_idx, new_f_idx, :]))\n",
    "            \n",
    "            total_reward += r\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "            \n",
    "            # End if wolf encountered\n",
    "            if board.grid[new_x][new_y] == 'wolf':\n",
    "                done = True\n",
    "                \n",
    "    return Q\n",
    "\n",
    "# --- Simulation Functions ---\n",
    "def run_game(board, Q=None, random_walk=False):\n",
    "    \"\"\"\n",
    "    Simulates a single game for either the RL agent or a random walk agent.\n",
    "    \n",
    "    Parameters:\n",
    "        board (Board): The game board.\n",
    "        Q (np.ndarray): The Q-table for the RL agent. If None, random walk is used.\n",
    "        random_walk (bool): If True, the agent takes random actions.\n",
    "    \n",
    "    Returns:\n",
    "        bool: True if the agent wins (defeats the wolf), False otherwise.\n",
    "    \"\"\"\n",
    "    # Initialize the agent's starting state\n",
    "    x, y = random.randint(0, board.width-1), random.randint(0, board.height-1)\n",
    "    state = (x, y, max_energy, 0)  # Start with full energy and no fatigue\n",
    "    steps = 0  # Step counter\n",
    "    \n",
    "    while steps < 50:  # Limit the number of steps per game\n",
    "        if random_walk:\n",
    "            # Random walk agent chooses actions randomly\n",
    "            action = random.choice(list(actions.keys()))\n",
    "        else:\n",
    "            # RL agent chooses actions based on the Q-table\n",
    "            action = choose_action(Q, state, epsilon=0)\n",
    "        \n",
    "        # Perform the chosen action and update the state\n",
    "        state, _ = step(state, action, board)\n",
    "        x, y, e, f = state\n",
    "        \n",
    "        # Check if the agent encounters the wolf\n",
    "        if board.grid[x][y] == 'wolf':\n",
    "            # Determine if the agent wins or loses based on energy and fatigue\n",
    "            if e >= required_energy and f <= max_fatigue_for_battle:\n",
    "                return True  # Win\n",
    "            else:\n",
    "                return False  # Lose\n",
    "        \n",
    "        steps += 1  # Increment step counter\n",
    "    \n",
    "    return False  # Return False if the game ends without encountering the wolf\n",
    "\n",
    "def compare_agents(n_games=100):\n",
    "    \"\"\"\n",
    "    Compares the performance of the RL agent and the random walk agent over multiple games.\n",
    "    \n",
    "    Parameters:\n",
    "        n_games (int): The number of games to simulate for each agent.\n",
    "    \n",
    "    Prints:\n",
    "        The number of wins for the RL agent and the random walk agent.\n",
    "    \"\"\"\n",
    "    # Initialize the game board\n",
    "    board = Board()\n",
    "    \n",
    "    # Train the RL agent using Q-learning\n",
    "    Q = q_learning(board, epochs=5000)\n",
    "    \n",
    "    # Simulate games for the RL agent\n",
    "    rl_wins = sum(run_game(board, Q) for _ in range(n_games))\n",
    "    \n",
    "    # Simulate games for the random walk agent\n",
    "    rw_wins = sum(run_game(board, random_walk=True) for _ in range(n_games))\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"RL Agent Wins: {rl_wins}/{n_games}\")\n",
    "    print(f\"Random Walk Wins: {rw_wins}/{n_games}\")\n",
    "\n",
    "# --- Run Comparison ---\n",
    "compare_agents()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Arewadatascience)",
   "language": "python",
   "name": "arewadatascience"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
